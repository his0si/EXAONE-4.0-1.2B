[INFO] Local base_model not found, using HF: LGAI-EXAONE/EXAONE-4.0-1.2B
======================================================================
  Knowledge Distillation from Large Teacher
  Teacher: LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct (NF4 4-bit)
  Student: LGAI-EXAONE/EXAONE-4.0-1.2B (BF16, all params)
  Output:  /home/sunwoo/quant/0208/checkpoints/kd_35_78b
======================================================================

[1/4] Loading tokenizer from /home/sunwoo/quant/0208/models/lane01_gptq_w4a16...
The tokenizer you are loading from '/home/sunwoo/quant/0208/models/lane01_gptq_w4a16' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.

[2/4] Loading training data...
[DATA] Loaded 20000 from /home/sunwoo/quant/0208/data/manta/train_50k.json
[DATA] Tokenizing 20000 samples, max_seq_length=512...

[3/4] Loading teacher: LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct (NF4 4-bit)...
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:01<00:07,  1.29s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:02<00:07,  1.51s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:04<00:06,  1.58s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:06<00:04,  1.59s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:07<00:03,  1.62s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:09<00:01,  1.61s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:09<00:00,  1.15s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:09<00:00,  1.39s/it]
  Teacher loaded: 5.17 GB

[4/4] Loading student: LGAI-EXAONE/EXAONE-4.0-1.2B (BF16)...
  Student loaded: 2.56 GB
  GPU: 7.8GB used / 25.4GB total

[KD] Configuration:
  Teacher: LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct
  Student: 1279M params (1279M trainable)
  Data: 20000 samples, batch=1, grad_accum=16
  Effective batch: 16
  Epochs: 3, Steps: 3750, Warmup: 187
  LR: 1e-05, Temperature: 2.0, Alpha(KD): 0.7

past_key_values should not be None in from_legacy_cache()
We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)
  E1 step 800/20000 | loss=1811.8919 kd=2587.6088 ce=1.8858 | lr=2.67e-06 | ETA=140min
  E1 step 1600/20000 | loss=1512.0186 kd=2159.2243 ce=1.8721 | lr=5.35e-06 | ETA=138min
  E1 step 2400/20000 | loss=1284.1573 kd=1833.7274 ce=1.8270 | lr=8.02e-06 | ETA=136min
  E1 step 3200/20000 | loss=1143.9711 kd=1633.4776 ce=1.7892 | lr=1.00e-05 | ETA=134min
  E1 step 4000/20000 | loss=1051.4589 kd=1501.3154 ce=1.7938 | lr=9.99e-06 | ETA=132min
  E1 step 4800/20000 | loss=988.3789 kd=1411.2091 ce=1.7752 | lr=9.98e-06 | ETA=130min
  E1 step 5600/20000 | loss=939.9680 kd=1342.0560 ce=1.7625 | lr=9.95e-06 | ETA=128min
  E1 step 6400/20000 | loss=901.9958 kd=1287.8140 ce=1.7536 | lr=9.91e-06 | ETA=127min
  E1 step 7200/20000 | loss=871.1560 kd=1243.7553 ce=1.7576 | lr=9.87e-06 | ETA=125min
  E1 step 8000/20000 | loss=846.2104 kd=1208.1190 ce=1.7569 | lr=9.81e-06 | ETA=123min
  E1 step 8800/20000 | loss=824.6587 kd=1177.3318 ce=1.7548 | lr=9.75e-06 | ETA=121min
  E1 step 9600/20000 | loss=806.8644 kd=1151.9118 ce=1.7539 | lr=9.67e-06 | ETA=119min
  E1 step 10400/20000 | loss=791.0748 kd=1129.3529 ce=1.7592 | lr=9.59e-06 | ETA=117min
  E1 step 11200/20000 | loss=778.5501 kd=1111.4634 ce=1.7522 | lr=9.50e-06 | ETA=115min
  E1 step 12000/20000 | loss=767.2001 kd=1095.2501 ce=1.7500 | lr=9.40e-06 | ETA=113min
  E1 step 12800/20000 | loss=756.9218 kd=1080.5652 ce=1.7540 | lr=9.29e-06 | ETA=111min
  E1 step 13600/20000 | loss=747.3051 kd=1066.8261 ce=1.7561 | lr=9.17e-06 | ETA=110min
  E1 step 14400/20000 | loss=738.5865 kd=1054.3729 ce=1.7516 | lr=9.04e-06 | ETA=108min
  E1 step 15200/20000 | loss=730.9620 kd=1043.4817 ce=1.7494 | lr=8.91e-06 | ETA=106min
  E1 step 16000/20000 | loss=723.5444 kd=1032.8851 ce=1.7495 | lr=8.77e-06 | ETA=104min
  E1 step 16800/20000 | loss=716.7844 kd=1023.2296 ce=1.7458 | lr=8.62e-06 | ETA=102min
  E1 step 17600/20000 | loss=710.7821 kd=1014.6563 ce=1.7422 | lr=8.47e-06 | ETA=100min
  E1 step 18400/20000 | loss=705.3692 kd=1006.9226 ce=1.7446 | lr=8.30e-06 | ETA=98min
  E1 step 19200/20000 | loss=700.5783 kd=1000.0799 ce=1.7414 | lr=8.13e-06 | ETA=96min
  E1 step 20000/20000 | loss=695.9018 kd=993.3981 ce=1.7437 | lr=7.96e-06 | ETA=94min

  Epoch 1/3 avg_loss=695.9018 (kd=993.3981, ce=1.7437) [47.2min elapsed]
  -> Saved best model (loss=695.9018)

  E2 step 800/20000 | loss=556.1169 kd=793.7261 ce=1.6956 | lr=7.78e-06 | ETA=93min
  E2 step 1600/20000 | loss=558.2705 kd=796.8099 ce=1.6786 | lr=7.59e-06 | ETA=91min
  E2 step 2400/20000 | loss=555.0810 kd=792.2392 ce=1.7117 | lr=7.40e-06 | ETA=89min
  E2 step 3200/20000 | loss=556.5841 kd=794.3861 ce=1.7128 | lr=7.21e-06 | ETA=87min
  E2 step 4000/20000 | loss=555.6755 kd=793.0912 ce=1.7058 | lr=7.01e-06 | ETA=85min
  E2 step 4800/20000 | loss=555.3417 kd=792.6166 ce=1.7004 | lr=6.80e-06 | ETA=83min
  E2 step 5600/20000 | loss=555.7535 kd=793.2060 ce=1.6976 | lr=6.60e-06 | ETA=81min
  E2 step 6400/20000 | loss=556.0463 kd=793.6264 ce=1.6928 | lr=6.39e-06 | ETA=79min
  E2 step 7200/20000 | loss=556.4026 kd=794.1369 ce=1.6892 | lr=6.17e-06 | ETA=77min
  E2 step 8000/20000 | loss=556.0068 kd=793.5705 ce=1.6915 | lr=5.96e-06 | ETA=76min
  E2 step 8800/20000 | loss=556.1407 kd=793.7622 ce=1.6905 | lr=5.74e-06 | ETA=74min
  E2 step 9600/20000 | loss=555.4831 kd=792.8228 ce=1.6904 | lr=5.52e-06 | ETA=72min
  E2 step 10400/20000 | loss=555.4221 kd=792.7347 ce=1.6926 | lr=5.30e-06 | ETA=70min
  E2 step 11200/20000 | loss=556.2171 kd=793.8695 ce=1.6947 | lr=5.08e-06 | ETA=68min
  E2 step 12000/20000 | loss=555.3143 kd=792.5801 ce=1.6942 | lr=4.86e-06 | ETA=66min
  E2 step 12800/20000 | loss=554.6899 kd=791.6895 ce=1.6907 | lr=4.64e-06 | ETA=64min
  E2 step 13600/20000 | loss=554.4189 kd=791.3021 ce=1.6916 | lr=4.42e-06 | ETA=62min
  E2 step 14400/20000 | loss=554.1919 kd=790.9791 ce=1.6887 | lr=4.20e-06 | ETA=60min
  E2 step 15200/20000 | loss=553.9463 kd=790.6272 ce=1.6910 | lr=3.99e-06 | ETA=59min
  E2 step 16000/20000 | loss=553.4604 kd=789.9324 ce=1.6925 | lr=3.77e-06 | ETA=57min
  E2 step 16800/20000 | loss=553.7684 kd=790.3735 ce=1.6897 | lr=3.56e-06 | ETA=55min
  E2 step 17600/20000 | loss=553.5491 kd=790.0606 ce=1.6892 | lr=3.35e-06 | ETA=53min
  E2 step 18400/20000 | loss=553.4133 kd=789.8671 ce=1.6879 | lr=3.14e-06 | ETA=51min
  E2 step 19200/20000 | loss=553.3846 kd=789.8243 ce=1.6917 | lr=2.94e-06 | ETA=49min
  E2 step 20000/20000 | loss=553.2839 kd=789.6814 ce=1.6896 | lr=2.74e-06 | ETA=47min

  Epoch 2/3 avg_loss=553.2839 (kd=789.6814, ce=1.6896) [94.5min elapsed]
  -> Saved best model (loss=553.2839)

  E3 step 800/20000 | loss=547.0139 kd=780.7103 ce=1.7223 | lr=2.55e-06 | ETA=45min
  E3 step 1600/20000 | loss=546.7756 kd=780.3732 ce=1.7146 | lr=2.36e-06 | ETA=43min
  E3 step 2400/20000 | loss=543.5718 kd=775.8042 ce=1.6964 | lr=2.17e-06 | ETA=42min
  E3 step 3200/20000 | loss=542.9583 kd=774.9332 ce=1.6838 | lr=1.99e-06 | ETA=40min
  E3 step 4000/20000 | loss=544.4193 kd=777.0200 ce=1.6842 | lr=1.82e-06 | ETA=38min
  E3 step 4800/20000 | loss=543.8688 kd=776.2320 ce=1.6879 | lr=1.65e-06 | ETA=36min
  E3 step 5600/20000 | loss=543.3445 kd=775.4848 ce=1.6838 | lr=1.49e-06 | ETA=34min
  E3 step 6400/20000 | loss=543.3061 kd=775.4368 ce=1.6676 | lr=1.34e-06 | ETA=32min
  E3 step 7200/20000 | loss=543.1717 kd=775.2447 ce=1.6681 | lr=1.19e-06 | ETA=30min
  E3 step 8000/20000 | loss=542.8000 kd=774.7137 ce=1.6680 | lr=1.05e-06 | ETA=28min
  E3 step 8800/20000 | loss=541.9947 kd=773.5649 ce=1.6644 | lr=9.23e-07 | ETA=26min
  E3 step 9600/20000 | loss=541.8067 kd=773.2970 ce=1.6628 | lr=7.99e-07 | ETA=25min
  E3 step 10400/20000 | loss=541.8984 kd=773.4275 ce=1.6638 | lr=6.84e-07 | ETA=23min
  E3 step 11200/20000 | loss=541.1419 kd=772.3422 ce=1.6745 | lr=5.77e-07 | ETA=21min
  E3 step 12000/20000 | loss=541.1409 kd=772.3407 ce=1.6746 | lr=4.78e-07 | ETA=19min
  E3 step 12800/20000 | loss=541.3044 kd=772.5730 ce=1.6778 | lr=3.88e-07 | ETA=17min
  E3 step 13600/20000 | loss=540.3745 kd=771.2451 ce=1.6766 | lr=3.08e-07 | ETA=15min
  E3 step 14400/20000 | loss=540.5723 kd=771.5280 ce=1.6757 | lr=2.36e-07 | ETA=13min
  E3 step 15200/20000 | loss=540.6093 kd=771.5804 ce=1.6766 | lr=1.74e-07 | ETA=11min
  E3 step 16000/20000 | loss=540.3248 kd=771.1747 ce=1.6750 | lr=1.21e-07 | ETA=9min
  E3 step 16800/20000 | loss=540.6733 kd=771.6721 ce=1.6763 | lr=7.75e-08 | ETA=8min
  E3 step 17600/20000 | loss=540.6240 kd=771.6008 ce=1.6780 | lr=4.37e-08 | ETA=6min
  E3 step 18400/20000 | loss=540.5841 kd=771.5431 ce=1.6798 | lr=1.94e-08 | ETA=4min
  E3 step 19200/20000 | loss=540.7443 kd=771.7711 ce=1.6816 | lr=4.86e-09 | ETA=2min
  E3 step 20000/20000 | loss=540.6238 kd=771.5998 ce=1.6798 | lr=0.00e+00 | ETA=0min

  Epoch 3/3 avg_loss=540.6238 (kd=771.5998, ce=1.6798) [141.8min elapsed]
  -> Saved best model (loss=540.6238)

[KD] Complete: 141.8 minutes, best_loss=540.6238

[DONE] KD model saved to /home/sunwoo/quant/0208/checkpoints/kd_35_78b
  Next: quantize to W4A16:
    python scripts/03_quantize.py --input-model checkpoints/kd_35_78b \
      --output models/kd_deep78b_w4a16 --scheme W4A16 --dampening 0.02
