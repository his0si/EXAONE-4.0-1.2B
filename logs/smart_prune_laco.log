`torch_dtype` is deprecated! Use `dtype` instead!
Traceback (most recent call last):
  File "/home/sunwoo/quant/0208/scripts/smart_prune.py", line 571, in <module>
    main()
  File "/home/sunwoo/quant/0208/scripts/smart_prune.py", line 553, in main
    quantize_w8a16(source_for_quant, quant_dir, tokenizer)
  File "/home/sunwoo/quant/0208/scripts/smart_prune.py", line 373, in quantize_w8a16
    model = AutoModelForCausalLM.from_pretrained(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sunwoo/miniconda3/envs/quant/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 549, in from_pretrained
    config, kwargs = AutoConfig.from_pretrained(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sunwoo/miniconda3/envs/quant/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 1372, in from_pretrained
    return config_class.from_dict(config_dict, **unused_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sunwoo/miniconda3/envs/quant/lib/python3.11/site-packages/transformers/configuration_utils.py", line 808, in from_dict
    config = cls(**config_dict)
             ^^^^^^^^^^^^^^^^^^
  File "/home/sunwoo/miniconda3/envs/quant/lib/python3.11/site-packages/transformers/models/exaone4/configuration_exaone4.py", line 215, in __init__
    layer_type_validation(self.layer_types, self.num_hidden_layers)
  File "/home/sunwoo/miniconda3/envs/quant/lib/python3.11/site-packages/transformers/configuration_utils.py", line 1377, in layer_type_validation
    raise ValueError(
ValueError: `num_hidden_layers` (28) must be equal to the number of layer types (30)

ERROR conda.cli.main_run:execute(127): `conda run python scripts/smart_prune.py --method laco --remove 2 --kd-samples 5000 --kd-epochs 3 --kd-seq 512 --kd-batch 2` failed. (See above for error)
============================================================
Smart Pruning: laco (remove 2)
============================================================
[ANALYSIS] Computing block-level importance (200 samples)...

 Layer   MHA cos   MLP cos  Full cos
--------------------------------------
    0   0.08736   0.84990   0.01094
    1   0.72073   0.89941   0.71350
    2   0.86051   0.92287   0.82398
    3   0.88146   0.92695   0.87530
    4   0.88745   0.93255   0.88416
    5   0.88622   0.91992   0.84952
    6   0.89079   0.93635   0.87251
    7   0.89038   0.93119   0.87624
    8   0.92693   0.94808   0.90727
    9   0.91672   0.93880   0.91103
   10   0.92682   0.94280   0.91927
   11   0.93885   0.95436   0.92842
   12   0.94127   0.95079   0.92093
   13   0.93595   0.95278   0.92999
   14   0.94832   0.95689   0.93798
   15   0.95753   0.95934   0.94115
   16   0.95530   0.96086   0.94005
   17   0.97524   0.96915   0.95393
   18   0.95594   0.96626   0.94072
   19   0.96536   0.96861   0.94707
   20   0.96997   0.97159   0.95069
   21   0.96584   0.96991   0.94558
   22   0.97330   0.96781   0.95073
   23   0.96338   0.96227   0.93519
   24   0.93562   0.94587   0.90794
   25   0.96632   0.94734   0.93503
   26   0.97325   0.96095   0.94411
   27   0.97258   0.96048   0.93676
   28   0.96680   0.93829   0.89278
   29   0.93936   0.84155   0.77975

[LaCo] Most redundant layers: [17, 22, 20, 19, 21, 26, 15, 18]
[LaCo] Will merge: [17, 22]

[LaCo] Merging layers: [17, 22]
  Merging layer 22 → layer 23
  Merging layer 17 → layer 18
[LaCo] Result: 28L, 1208M params

[SAVE] Pruned model: /home/sunwoo/quant/0208/checkpoints/laco_28L (2314.9 MB)

============================================================
Phase 2: Knowledge Distillation
============================================================

[KD] Loading training data...
[KD] Student: 28L, 1208M trainable params
[KD] Data: 5000 samples, batch=2x4, epochs=3
[KD] LR=2e-05, T=2.0, alpha=0.7, total_steps=1875
  E1 step 200/2500 | loss=631.9430 | 0.5min
  E1 step 400/2500 | loss=506.1634 | 0.9min
  E1 step 600/2500 | loss=435.8329 | 1.4min
  E1 step 800/2500 | loss=392.8330 | 1.9min
  E1 step 1000/2500 | loss=362.2644 | 2.3min
  E1 step 1200/2500 | loss=340.5824 | 2.8min
  E1 step 1400/2500 | loss=323.3008 | 3.2min
  E1 step 1600/2500 | loss=309.9680 | 3.7min
  E1 step 1800/2500 | loss=298.4693 | 4.2min
  E1 step 2000/2500 | loss=289.0670 | 4.6min
  E1 step 2200/2500 | loss=280.6547 | 5.1min
  E1 step 2400/2500 | loss=273.6433 | 5.6min
  Epoch 1/3 avg_loss=270.3440 [5.8min]
  Saved best (loss=270.3440)
  E2 step 200/2500 | loss=167.4104 | 6.3min
  E2 step 400/2500 | loss=165.4494 | 6.8min
  E2 step 600/2500 | loss=164.5165 | 7.2min
  E2 step 800/2500 | loss=163.4167 | 7.7min
  E2 step 1000/2500 | loss=162.6121 | 8.2min
  E2 step 1200/2500 | loss=161.8615 | 8.6min
  E2 step 1400/2500 | loss=161.0794 | 9.1min
  E2 step 1600/2500 | loss=160.6767 | 9.5min
  E2 step 1800/2500 | loss=159.8962 | 10.0min
  E2 step 2000/2500 | loss=159.3289 | 10.5min
  E2 step 2200/2500 | loss=158.8820 | 10.9min
  E2 step 2400/2500 | loss=158.1843 | 11.4min
  Epoch 2/3 avg_loss=157.8953 [11.6min]
  Saved best (loss=157.8953)
  E3 step 200/2500 | loss=138.8093 | 12.1min
  E3 step 400/2500 | loss=138.1712 | 12.6min
  E3 step 600/2500 | loss=138.2458 | 13.1min
  E3 step 800/2500 | loss=137.8558 | 13.5min
  E3 step 1000/2500 | loss=138.1026 | 14.0min
  E3 step 1200/2500 | loss=138.1064 | 14.5min
  E3 step 1400/2500 | loss=137.8004 | 14.9min
  E3 step 1600/2500 | loss=137.9025 | 15.4min
  E3 step 1800/2500 | loss=137.9789 | 15.8min
  E3 step 2000/2500 | loss=137.8972 | 16.3min
  E3 step 2200/2500 | loss=137.8531 | 16.8min
  E3 step 2400/2500 | loss=137.6915 | 17.2min
  Epoch 3/3 avg_loss=137.6019 [17.5min]
  Saved best (loss=137.6019)

============================================================
Phase 3: W8A16 Quantization
============================================================

