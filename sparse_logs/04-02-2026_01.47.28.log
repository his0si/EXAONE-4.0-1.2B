2026-02-04 01:47:28.379 | INFO     | llmcompressor.metrics.logger:_create_default_logger:357 - Logging all LLM Compressor modifier-level logs to sparse_logs/04-02-2026_01.47.28.log
2026-02-04 01:47:28.379 | DEBUG    | llmcompressor.core.lifecycle:initialize:92 - Initializing compression lifecycle
2026-02-04 01:47:28.379 | INFO     | llmcompressor.recipe.recipe:from_modifiers:68 - Creating recipe from modifiers
2026-02-04 01:47:28.397 | DEBUG    | llmcompressor.core.lifecycle:initialize:105 - Initialized modifier: config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=False ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False
2026-02-04 01:47:28.397 | INFO     | llmcompressor.core.lifecycle:initialize:110 - Compression lifecycle initialized for 1 modifiers
2026-02-04 01:47:28.397 | INFO     | llmcompressor.pipelines.independent.pipeline:IndependentPipeline:43 - Inferred `SequentialPipeline` for `GPTQModifier`
2026-02-04 01:47:29.799 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2026-02-04 01:47:29.799 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_0(input_ids, inputs_embeds):
    if (input_ids is None) ^ (inputs_embeds is not None):
        raise ValueError('You must specify exactly one of input_ids or inputs_embeds')
    return ()
2026-02-04 01:47:29.799 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2026-02-04 01:47:29.799 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2026-02-04 01:47:29.800 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_1(input_ids, inputs_embeds):
    if inputs_embeds is None:
        inputs_embeds = self.embed_tokens(input_ids)
    return (inputs_embeds,)
2026-02-04 01:47:29.800 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2026-02-04 01:47:29.800 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2026-02-04 01:47:29.800 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_2(past_key_values, use_cache):
    if use_cache and past_key_values is None:
        past_key_values = DynamicCache()
    return (past_key_values,)
2026-02-04 01:47:29.800 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2026-02-04 01:47:29.800 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2026-02-04 01:47:29.800 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_3(cache_position, inputs_embeds, past_key_values, *, past_seen_tokens=None):
    if cache_position is None:
        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        cache_position = torch.arange(past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device)
    return (cache_position, past_seen_tokens)
2026-02-04 01:47:29.800 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2026-02-04 01:47:29.800 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2026-02-04 01:47:29.800 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_4(cache_position, position_ids):
    if position_ids is None:
        position_ids = cache_position.unsqueeze(0)
    return (position_ids,)
2026-02-04 01:47:29.800 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2026-02-04 01:47:29.800 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2026-02-04 01:47:29.801 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_5(attention_mask, cache_position, inputs_embeds, past_key_values, position_ids, *, causal_mask_mapping=None, mask_kwargs=None):
    if not isinstance((causal_mask_mapping := attention_mask), dict):
        mask_kwargs = {'config': self.config, 'input_embeds': inputs_embeds, 'attention_mask': attention_mask, 'cache_position': cache_position, 'past_key_values': past_key_values, 'position_ids': position_ids}
        causal_mask_mapping = {'full_attention': create_causal_mask(**mask_kwargs)}
        if 'sliding_attention' in self.config.layer_types:
            causal_mask_mapping['sliding_attention'] = create_sliding_window_causal_mask(**mask_kwargs)
    return (causal_mask_mapping, mask_kwargs)
2026-02-04 01:47:29.801 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2026-02-04 01:47:29.804 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2026-02-04 01:47:29.804 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_0(kwargs, labels, logits, loss):
    if labels is not None:
        loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)
    return (loss,)
2026-02-04 01:47:29.804 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2026-02-04 01:47:29.845 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.CALIBRATION_EPOCH_START
2026-02-04 01:47:29.852 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef75d80>
2026-02-04 01:47:29.852 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef75ea0>
2026-02-04 01:47:29.852 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef76020>
2026-02-04 01:47:29.852 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef761d0>
2026-02-04 01:47:29.852 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef76920>
2026-02-04 01:47:29.852 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef77f40>
2026-02-04 01:47:29.852 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef77ee0>
2026-02-04 01:47:29.852 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef77df0>
2026-02-04 01:47:29.852 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef77bb0>
2026-02-04 01:47:29.852 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef77b20>
2026-02-04 01:47:29.852 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef76aa0>
2026-02-04 01:47:29.852 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef76770>
2026-02-04 01:47:29.852 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef172e0>
2026-02-04 01:47:29.852 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef17310>
2026-02-04 01:47:29.852 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef17490>
2026-02-04 01:47:29.852 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef176d0>
2026-02-04 01:47:29.852 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef17700>
2026-02-04 01:47:29.852 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef15630>
2026-02-04 01:47:29.852 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef143a0>
2026-02-04 01:47:29.852 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef15d50>
2026-02-04 01:47:29.852 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef15c30>
2026-02-04 01:47:29.852 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef15b40>
2026-02-04 01:47:29.852 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef15cf0>
2026-02-04 01:47:29.852 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef15e10>
2026-02-04 01:47:29.852 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef15090>
2026-02-04 01:47:29.852 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef15750>
2026-02-04 01:47:29.852 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef168c0>
2026-02-04 01:47:29.853 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef166e0>
2026-02-04 01:47:29.853 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef165f0>
2026-02-04 01:47:29.853 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef16560>
2026-02-04 01:47:29.853 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef164a0>
2026-02-04 01:47:29.853 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef16740>
2026-02-04 01:47:29.853 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef163b0>
2026-02-04 01:47:29.853 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef16320>
2026-02-04 01:47:29.853 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef161a0>
2026-02-04 01:47:29.853 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef16260>
2026-02-04 01:47:29.853 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef16110>
2026-02-04 01:47:29.853 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef15f90>
2026-02-04 01:47:29.853 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef16080>
2026-02-04 01:47:29.853 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef15ed0>
2026-02-04 01:47:29.853 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef15180>
2026-02-04 01:47:29.853 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef15240>
2026-02-04 01:47:29.853 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef142b0>
2026-02-04 01:47:29.853 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef15330>
2026-02-04 01:47:29.853 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef15a50>
2026-02-04 01:47:29.853 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef15990>
2026-02-04 01:47:29.853 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef158a0>
2026-02-04 01:47:29.853 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef15810>
2026-02-04 01:47:29.853 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef16aa0>
2026-02-04 01:47:29.853 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef169b0>
2026-02-04 01:47:29.853 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef16920>
2026-02-04 01:47:29.853 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef16b00>
2026-02-04 01:47:29.853 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef17e80>
2026-02-04 01:47:29.853 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef17d90>
2026-02-04 01:47:29.853 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef17d30>
2026-02-04 01:47:29.853 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef17cd0>
2026-02-04 01:47:29.853 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef17b80>
2026-02-04 01:47:29.854 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef17ac0>
2026-02-04 01:47:29.854 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef17a00>
2026-02-04 01:47:29.854 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef17970>
2026-02-04 01:47:29.854 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef17850>
2026-02-04 01:47:29.854 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef177f0>
2026-02-04 01:47:29.854 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef16bc0>
2026-02-04 01:47:29.854 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef15660>
2026-02-04 01:47:29.854 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef14250>
2026-02-04 01:47:29.854 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a68ef17790>
2026-02-04 01:47:29.854 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818c0a0>
2026-02-04 01:47:29.854 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818c190>
2026-02-04 01:47:29.854 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818c220>
2026-02-04 01:47:29.854 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818c2e0>
2026-02-04 01:47:29.854 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818c3d0>
2026-02-04 01:47:29.854 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818c460>
2026-02-04 01:47:29.854 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818c520>
2026-02-04 01:47:29.854 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818c5e0>
2026-02-04 01:47:29.854 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818c6d0>
2026-02-04 01:47:29.854 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818c760>
2026-02-04 01:47:29.854 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818c820>
2026-02-04 01:47:29.854 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818c910>
2026-02-04 01:47:29.854 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818c9a0>
2026-02-04 01:47:29.854 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818ca60>
2026-02-04 01:47:29.854 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818cb20>
2026-02-04 01:47:29.854 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818cc10>
2026-02-04 01:47:29.854 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818cca0>
2026-02-04 01:47:29.854 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818cd60>
2026-02-04 01:47:29.854 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818ce50>
2026-02-04 01:47:29.854 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818cee0>
2026-02-04 01:47:29.854 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818cfa0>
2026-02-04 01:47:29.854 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818d060>
2026-02-04 01:47:29.854 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818d150>
2026-02-04 01:47:29.855 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818d1e0>
2026-02-04 01:47:29.855 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818d2a0>
2026-02-04 01:47:29.855 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818d390>
2026-02-04 01:47:29.855 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818d420>
2026-02-04 01:47:29.855 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818d4e0>
2026-02-04 01:47:29.855 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818d5a0>
2026-02-04 01:47:29.855 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818d690>
2026-02-04 01:47:29.855 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818d720>
2026-02-04 01:47:29.855 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818d7e0>
2026-02-04 01:47:29.855 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818d8d0>
2026-02-04 01:47:29.855 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818d960>
2026-02-04 01:47:29.855 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818da20>
2026-02-04 01:47:29.855 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818dae0>
2026-02-04 01:47:29.855 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818dbd0>
2026-02-04 01:47:29.855 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818dc60>
2026-02-04 01:47:29.855 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818dd20>
2026-02-04 01:47:29.855 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818de10>
2026-02-04 01:47:29.855 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818dea0>
2026-02-04 01:47:29.855 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818df60>
2026-02-04 01:47:29.855 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818e020>
2026-02-04 01:47:29.855 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818e110>
2026-02-04 01:47:29.855 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818e1a0>
2026-02-04 01:47:29.855 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818e260>
2026-02-04 01:47:29.855 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818e350>
2026-02-04 01:47:29.855 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818e3e0>
2026-02-04 01:47:29.855 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818e4a0>
2026-02-04 01:47:29.855 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818e560>
2026-02-04 01:47:29.855 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818e650>
2026-02-04 01:47:29.855 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818e6e0>
2026-02-04 01:47:29.855 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818e7a0>
2026-02-04 01:47:29.856 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818e890>
2026-02-04 01:47:29.856 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818e920>
2026-02-04 01:47:29.856 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818e9e0>
2026-02-04 01:47:29.856 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818eaa0>
2026-02-04 01:47:29.856 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818eb90>
2026-02-04 01:47:29.856 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818ec20>
2026-02-04 01:47:29.856 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818ece0>
2026-02-04 01:47:29.856 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818edd0>
2026-02-04 01:47:29.856 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818ee60>
2026-02-04 01:47:29.856 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818ef20>
2026-02-04 01:47:29.856 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818efe0>
2026-02-04 01:47:29.856 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818f0d0>
2026-02-04 01:47:29.856 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818f160>
2026-02-04 01:47:29.856 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818f220>
2026-02-04 01:47:29.856 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818f310>
2026-02-04 01:47:29.856 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818f3a0>
2026-02-04 01:47:29.856 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818f460>
2026-02-04 01:47:29.856 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818f520>
2026-02-04 01:47:29.856 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818f610>
2026-02-04 01:47:29.856 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818f6a0>
2026-02-04 01:47:29.856 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818f760>
2026-02-04 01:47:29.856 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818f850>
2026-02-04 01:47:29.856 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818f8e0>
2026-02-04 01:47:29.856 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818f9a0>
2026-02-04 01:47:29.856 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818fa60>
2026-02-04 01:47:29.856 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818fb50>
2026-02-04 01:47:29.856 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818fbe0>
2026-02-04 01:47:29.856 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818fca0>
2026-02-04 01:47:29.856 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818fd90>
2026-02-04 01:47:29.856 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818fe20>
2026-02-04 01:47:29.856 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818fee0>
2026-02-04 01:47:29.856 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e818ffa0>
2026-02-04 01:47:29.857 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e81ac0d0>
2026-02-04 01:47:29.857 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e81ac160>
2026-02-04 01:47:29.857 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e81ac220>
2026-02-04 01:47:29.857 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e81ac310>
2026-02-04 01:47:29.857 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e81ac3a0>
2026-02-04 01:47:29.857 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e81ac460>
2026-02-04 01:47:29.857 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e81ac520>
2026-02-04 01:47:29.857 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e81ac610>
2026-02-04 01:47:29.857 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e81ac6a0>
2026-02-04 01:47:29.857 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e81ac760>
2026-02-04 01:47:29.857 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e81ac850>
2026-02-04 01:47:29.857 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e81ac8e0>
2026-02-04 01:47:29.857 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e81ac9a0>
2026-02-04 01:47:29.857 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e81aca60>
2026-02-04 01:47:29.857 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e81acb50>
2026-02-04 01:47:29.857 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e81acbe0>
2026-02-04 01:47:29.857 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e81acca0>
2026-02-04 01:47:29.857 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e81acd90>
2026-02-04 01:47:29.857 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e81ace20>
2026-02-04 01:47:29.857 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e81acee0>
2026-02-04 01:47:29.857 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e81acfa0>
2026-02-04 01:47:29.857 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e81ad090>
2026-02-04 01:47:29.857 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e81ad120>
2026-02-04 01:47:29.857 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e81ad1e0>
2026-02-04 01:47:29.857 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e81ad2d0>
2026-02-04 01:47:29.857 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e81ad360>
2026-02-04 01:47:29.857 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e81ad420>
2026-02-04 01:47:29.857 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e81ad4e0>
2026-02-04 01:47:29.857 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e81ad5d0>
2026-02-04 01:47:29.857 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e81ad660>
2026-02-04 01:47:29.857 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e81ad720>
2026-02-04 01:47:29.857 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e81ad810>
2026-02-04 01:47:29.857 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e81ad8a0>
2026-02-04 01:47:29.858 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e81ad960>
2026-02-04 01:47:29.858 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e81ada20>
2026-02-04 01:47:29.858 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e81adb10>
2026-02-04 01:47:29.858 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e81adba0>
2026-02-04 01:47:29.858 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e81adc60>
2026-02-04 01:47:29.858 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e81add50>
2026-02-04 01:47:29.858 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e81adde0>
2026-02-04 01:47:29.858 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e81adea0>
2026-02-04 01:47:29.858 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e81adf60>
2026-02-04 01:47:29.858 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e81ae050>
2026-02-04 01:47:29.858 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e81ae0e0>
2026-02-04 01:47:29.858 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e81ae1a0>
2026-02-04 01:47:29.858 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e81ae290>
2026-02-04 01:47:29.858 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e81ae320>
2026-02-04 01:47:29.858 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e81ae3e0>
2026-02-04 01:47:29.858 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e81ae4a0>
2026-02-04 01:47:29.858 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e81ae590>
2026-02-04 01:47:29.858 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e81ae620>
2026-02-04 01:47:29.858 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e81ae6e0>
2026-02-04 01:47:29.858 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e81ae7d0>
2026-02-04 01:47:29.858 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e81ae860>
2026-02-04 01:47:29.858 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e81ae920>
2026-02-04 01:47:29.858 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e81ae9e0>
2026-02-04 01:47:29.858 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e81aead0>
2026-02-04 01:47:29.858 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e81aeb60>
2026-02-04 01:47:29.858 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x73a6e81aec20>
2026-02-04 01:47:29.858 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False
2026-02-04 01:47:32.135 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-02-04 01:47:32.135 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.0.self_attn.q_proj using 256 samples
2026-02-04 01:47:32.635 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.50s
2026-02-04 01:47:32.635 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 1.11
2026-02-04 01:47:32.635 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 9.26% | total memory: 17 GB
2026-02-04 01:47:32.635 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 8.486912 MB
2026-02-04 01:47:32.636 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.0.self_attn.k_proj using 256 samples
2026-02-04 01:47:32.989 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:47:32.989 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 0.33
2026-02-04 01:47:32.989 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 9.26% | total memory: 17 GB
2026-02-04 01:47:32.989 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 2.121728 MB
2026-02-04 01:47:32.989 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.0.self_attn.v_proj using 256 samples
2026-02-04 01:47:33.336 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:47:33.336 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 0.19
2026-02-04 01:47:33.336 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 9.26% | total memory: 17 GB
2026-02-04 01:47:33.336 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 2.121728 MB
2026-02-04 01:47:33.336 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.0.self_attn.o_proj using 256 samples
2026-02-04 01:47:33.690 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:47:33.690 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 0.04
2026-02-04 01:47:33.690 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 9.26% | total memory: 17 GB
2026-02-04 01:47:33.690 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 8.486912 MB
2026-02-04 01:47:33.690 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.0.mlp.gate_proj using 256 samples
2026-02-04 01:47:34.061 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.37s
2026-02-04 01:47:34.061 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 4.42
2026-02-04 01:47:34.061 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 9.26% | total memory: 17 GB
2026-02-04 01:47:34.061 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:47:34.061 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.0.mlp.up_proj using 256 samples
2026-02-04 01:47:34.430 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.37s
2026-02-04 01:47:34.431 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 3.42
2026-02-04 01:47:34.431 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 9.26% | total memory: 17 GB
2026-02-04 01:47:34.431 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:47:34.431 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.0.mlp.down_proj using 256 samples
2026-02-04 01:47:35.166 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.73s
2026-02-04 01:47:35.166 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 0.00
2026-02-04 01:47:35.166 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 9.66% | total memory: 17 GB
2026-02-04 01:47:35.166 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:47:35.166 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False
2026-02-04 01:47:37.603 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-02-04 01:47:37.603 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.1.self_attn.q_proj using 256 samples
2026-02-04 01:47:37.964 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.36s
2026-02-04 01:47:37.964 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 4.76
2026-02-04 01:47:37.964 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:47:37.964 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 8.486912 MB
2026-02-04 01:47:37.964 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.1.self_attn.k_proj using 256 samples
2026-02-04 01:47:38.309 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.34s
2026-02-04 01:47:38.310 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 1.36
2026-02-04 01:47:38.310 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:47:38.310 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 2.121728 MB
2026-02-04 01:47:38.310 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.1.self_attn.v_proj using 256 samples
2026-02-04 01:47:38.656 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:47:38.657 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 1.27
2026-02-04 01:47:38.657 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:47:38.657 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 2.121728 MB
2026-02-04 01:47:38.657 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.1.self_attn.o_proj using 256 samples
2026-02-04 01:47:39.028 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.37s
2026-02-04 01:47:39.028 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 0.21
2026-02-04 01:47:39.028 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:47:39.028 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 8.486912 MB
2026-02-04 01:47:39.029 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.1.mlp.gate_proj using 256 samples
2026-02-04 01:47:39.397 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.37s
2026-02-04 01:47:39.398 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 17.34
2026-02-04 01:47:39.398 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:47:39.398 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:47:39.398 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.1.mlp.up_proj using 256 samples
2026-02-04 01:47:39.768 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.37s
2026-02-04 01:47:39.768 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 15.74
2026-02-04 01:47:39.768 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:47:39.768 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:47:39.768 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.1.mlp.down_proj using 256 samples
2026-02-04 01:47:40.505 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.74s
2026-02-04 01:47:40.505 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 0.01
2026-02-04 01:47:40.505 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 7.20% | total memory: 17 GB
2026-02-04 01:47:40.505 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:47:40.505 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False
2026-02-04 01:47:42.610 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-02-04 01:47:42.610 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.2.self_attn.q_proj using 256 samples
2026-02-04 01:47:42.969 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.36s
2026-02-04 01:47:42.970 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 12.94
2026-02-04 01:47:42.970 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:47:42.970 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 8.486912 MB
2026-02-04 01:47:42.970 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.2.self_attn.k_proj using 256 samples
2026-02-04 01:47:43.317 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:47:43.317 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 3.63
2026-02-04 01:47:43.317 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:47:43.317 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 2.121728 MB
2026-02-04 01:47:43.317 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.2.self_attn.v_proj using 256 samples
2026-02-04 01:47:43.666 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:47:43.667 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 3.57
2026-02-04 01:47:43.667 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:47:43.667 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 2.121728 MB
2026-02-04 01:47:43.667 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.2.self_attn.o_proj using 256 samples
2026-02-04 01:47:44.018 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:47:44.018 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 0.57
2026-02-04 01:47:44.019 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:47:44.019 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 8.486912 MB
2026-02-04 01:47:44.019 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.2.mlp.gate_proj using 256 samples
2026-02-04 01:47:44.390 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.37s
2026-02-04 01:47:44.391 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 41.22
2026-02-04 01:47:44.391 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:47:44.391 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:47:44.391 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.2.mlp.up_proj using 256 samples
2026-02-04 01:47:44.764 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.37s
2026-02-04 01:47:44.764 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 36.93
2026-02-04 01:47:44.764 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:47:44.764 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:47:44.764 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.2.mlp.down_proj using 256 samples
2026-02-04 01:47:45.504 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.74s
2026-02-04 01:47:45.504 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 0.04
2026-02-04 01:47:45.504 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 7.20% | total memory: 17 GB
2026-02-04 01:47:45.504 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:47:45.504 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False
2026-02-04 01:47:47.461 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-02-04 01:47:47.461 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.3.self_attn.q_proj using 256 samples
2026-02-04 01:47:47.823 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.36s
2026-02-04 01:47:47.824 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 26.43
2026-02-04 01:47:47.824 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:47:47.824 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 8.486912 MB
2026-02-04 01:47:47.824 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.3.self_attn.k_proj using 256 samples
2026-02-04 01:47:48.171 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:47:48.171 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 7.47
2026-02-04 01:47:48.171 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:47:48.171 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 2.121728 MB
2026-02-04 01:47:48.171 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.3.self_attn.v_proj using 256 samples
2026-02-04 01:47:48.521 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:47:48.521 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 6.71
2026-02-04 01:47:48.521 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:47:48.521 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 2.121728 MB
2026-02-04 01:47:48.521 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.3.self_attn.o_proj using 256 samples
2026-02-04 01:47:48.870 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:47:48.871 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 0.48
2026-02-04 01:47:48.871 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:47:48.871 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 8.486912 MB
2026-02-04 01:47:48.871 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.3.mlp.gate_proj using 256 samples
2026-02-04 01:47:49.257 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.39s
2026-02-04 01:47:49.257 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 77.86
2026-02-04 01:47:49.258 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:47:49.258 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:47:49.258 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.3.mlp.up_proj using 256 samples
2026-02-04 01:47:49.633 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.38s
2026-02-04 01:47:49.633 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 67.79
2026-02-04 01:47:49.633 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:47:49.633 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:47:49.633 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.3.mlp.down_proj using 256 samples
2026-02-04 01:47:50.367 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.73s
2026-02-04 01:47:50.367 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 0.12
2026-02-04 01:47:50.367 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 7.20% | total memory: 17 GB
2026-02-04 01:47:50.367 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:47:50.367 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False
2026-02-04 01:47:52.323 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-02-04 01:47:52.324 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.4.self_attn.q_proj using 256 samples
2026-02-04 01:47:52.682 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.36s
2026-02-04 01:47:52.682 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 50.37
2026-02-04 01:47:52.682 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:47:52.682 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 8.486912 MB
2026-02-04 01:47:52.682 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.4.self_attn.k_proj using 256 samples
2026-02-04 01:47:53.028 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:47:53.029 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 13.96
2026-02-04 01:47:53.029 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:47:53.029 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 2.121728 MB
2026-02-04 01:47:53.029 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.4.self_attn.v_proj using 256 samples
2026-02-04 01:47:53.374 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.34s
2026-02-04 01:47:53.374 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 12.77
2026-02-04 01:47:53.374 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:47:53.374 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 2.121728 MB
2026-02-04 01:47:53.374 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.4.self_attn.o_proj using 256 samples
2026-02-04 01:47:53.725 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:47:53.725 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 1.30
2026-02-04 01:47:53.725 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:47:53.725 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 8.486912 MB
2026-02-04 01:47:53.725 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.4.mlp.gate_proj using 256 samples
2026-02-04 01:47:54.098 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.37s
2026-02-04 01:47:54.098 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 147.17
2026-02-04 01:47:54.099 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:47:54.099 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:47:54.099 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.4.mlp.up_proj using 256 samples
2026-02-04 01:47:54.471 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.37s
2026-02-04 01:47:54.472 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 126.64
2026-02-04 01:47:54.472 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:47:54.472 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:47:54.472 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.4.mlp.down_proj using 256 samples
2026-02-04 01:47:55.203 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.73s
2026-02-04 01:47:55.203 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 0.30
2026-02-04 01:47:55.203 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 7.20% | total memory: 17 GB
2026-02-04 01:47:55.203 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:47:55.204 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False
2026-02-04 01:47:57.162 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-02-04 01:47:57.162 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.5.self_attn.q_proj using 256 samples
2026-02-04 01:47:57.524 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.36s
2026-02-04 01:47:57.524 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 81.36
2026-02-04 01:47:57.524 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:47:57.524 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 8.486912 MB
2026-02-04 01:47:57.524 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.5.self_attn.k_proj using 256 samples
2026-02-04 01:47:57.870 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:47:57.871 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 23.91
2026-02-04 01:47:57.871 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:47:57.871 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 2.121728 MB
2026-02-04 01:47:57.871 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.5.self_attn.v_proj using 256 samples
2026-02-04 01:47:58.216 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:47:58.216 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 20.82
2026-02-04 01:47:58.216 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:47:58.216 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 2.121728 MB
2026-02-04 01:47:58.217 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.5.self_attn.o_proj using 256 samples
2026-02-04 01:47:58.570 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:47:58.570 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 2.42
2026-02-04 01:47:58.570 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:47:58.570 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 8.486912 MB
2026-02-04 01:47:58.570 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.5.mlp.gate_proj using 256 samples
2026-02-04 01:47:58.942 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.37s
2026-02-04 01:47:58.942 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 294.44
2026-02-04 01:47:58.943 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:47:58.943 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:47:58.943 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.5.mlp.up_proj using 256 samples
2026-02-04 01:47:59.315 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.37s
2026-02-04 01:47:59.316 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 196.74
2026-02-04 01:47:59.316 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:47:59.316 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:47:59.316 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.5.mlp.down_proj using 256 samples
2026-02-04 01:48:00.074 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.76s
2026-02-04 01:48:00.075 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 1.09
2026-02-04 01:48:00.075 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 7.20% | total memory: 17 GB
2026-02-04 01:48:00.075 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:48:00.075 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False
2026-02-04 01:48:02.036 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-02-04 01:48:02.036 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.6.self_attn.q_proj using 256 samples
2026-02-04 01:48:02.397 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.36s
2026-02-04 01:48:02.397 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 118.57
2026-02-04 01:48:02.397 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:02.397 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 8.486912 MB
2026-02-04 01:48:02.398 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.6.self_attn.k_proj using 256 samples
2026-02-04 01:48:02.744 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:48:02.744 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 32.60
2026-02-04 01:48:02.744 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:02.744 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 2.121728 MB
2026-02-04 01:48:02.744 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.6.self_attn.v_proj using 256 samples
2026-02-04 01:48:03.089 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.34s
2026-02-04 01:48:03.090 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 32.53
2026-02-04 01:48:03.090 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:03.090 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 2.121728 MB
2026-02-04 01:48:03.090 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.6.self_attn.o_proj using 256 samples
2026-02-04 01:48:03.442 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:48:03.442 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 7.81
2026-02-04 01:48:03.442 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:03.442 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 8.486912 MB
2026-02-04 01:48:03.442 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.6.mlp.gate_proj using 256 samples
2026-02-04 01:48:03.813 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.37s
2026-02-04 01:48:03.813 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 437.51
2026-02-04 01:48:03.813 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:03.813 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:48:03.813 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.6.mlp.up_proj using 256 samples
2026-02-04 01:48:04.188 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.37s
2026-02-04 01:48:04.189 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 304.27
2026-02-04 01:48:04.189 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:04.189 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:48:04.189 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.6.mlp.down_proj using 256 samples
2026-02-04 01:48:04.923 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.73s
2026-02-04 01:48:04.924 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 2.45
2026-02-04 01:48:04.924 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 7.20% | total memory: 17 GB
2026-02-04 01:48:04.924 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:48:04.924 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False
2026-02-04 01:48:06.883 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-02-04 01:48:06.883 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.7.self_attn.q_proj using 256 samples
2026-02-04 01:48:07.245 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.36s
2026-02-04 01:48:07.246 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 178.74
2026-02-04 01:48:07.246 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:07.246 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 8.486912 MB
2026-02-04 01:48:07.246 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.7.self_attn.k_proj using 256 samples
2026-02-04 01:48:07.593 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:48:07.593 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 50.33
2026-02-04 01:48:07.593 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:07.593 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 2.121728 MB
2026-02-04 01:48:07.593 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.7.self_attn.v_proj using 256 samples
2026-02-04 01:48:07.940 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:48:07.940 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 45.53
2026-02-04 01:48:07.940 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:07.940 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 2.121728 MB
2026-02-04 01:48:07.940 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.7.self_attn.o_proj using 256 samples
2026-02-04 01:48:08.294 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:48:08.294 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 9.15
2026-02-04 01:48:08.294 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:08.294 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 8.486912 MB
2026-02-04 01:48:08.294 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.7.mlp.gate_proj using 256 samples
2026-02-04 01:48:08.668 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.37s
2026-02-04 01:48:08.668 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 680.03
2026-02-04 01:48:08.668 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:08.668 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:48:08.668 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.7.mlp.up_proj using 256 samples
2026-02-04 01:48:09.041 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.37s
2026-02-04 01:48:09.041 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 432.98
2026-02-04 01:48:09.041 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:09.041 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:48:09.041 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.7.mlp.down_proj using 256 samples
2026-02-04 01:48:09.787 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.75s
2026-02-04 01:48:09.787 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 5.10
2026-02-04 01:48:09.787 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 7.20% | total memory: 17 GB
2026-02-04 01:48:09.787 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:48:09.788 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False
2026-02-04 01:48:11.753 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-02-04 01:48:11.753 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.8.self_attn.q_proj using 256 samples
2026-02-04 01:48:12.113 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.36s
2026-02-04 01:48:12.113 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 195.52
2026-02-04 01:48:12.113 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:12.113 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 8.486912 MB
2026-02-04 01:48:12.114 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.8.self_attn.k_proj using 256 samples
2026-02-04 01:48:12.458 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.34s
2026-02-04 01:48:12.458 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 55.74
2026-02-04 01:48:12.458 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:12.459 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 2.121728 MB
2026-02-04 01:48:12.459 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.8.self_attn.v_proj using 256 samples
2026-02-04 01:48:12.803 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.34s
2026-02-04 01:48:12.803 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 55.68
2026-02-04 01:48:12.804 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:12.804 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 2.121728 MB
2026-02-04 01:48:12.804 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.8.self_attn.o_proj using 256 samples
2026-02-04 01:48:13.155 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:48:13.155 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 9.81
2026-02-04 01:48:13.155 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:13.156 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 8.486912 MB
2026-02-04 01:48:13.156 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.8.mlp.gate_proj using 256 samples
2026-02-04 01:48:13.530 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.37s
2026-02-04 01:48:13.530 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 628.32
2026-02-04 01:48:13.530 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:13.530 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:48:13.530 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.8.mlp.up_proj using 256 samples
2026-02-04 01:48:13.900 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.37s
2026-02-04 01:48:13.900 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 484.48
2026-02-04 01:48:13.901 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:13.901 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:48:13.901 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.8.mlp.down_proj using 256 samples
2026-02-04 01:48:14.633 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.73s
2026-02-04 01:48:14.633 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 5.66
2026-02-04 01:48:14.633 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 7.20% | total memory: 17 GB
2026-02-04 01:48:14.634 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:48:14.634 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False
2026-02-04 01:48:16.595 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-02-04 01:48:16.595 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.9.self_attn.q_proj using 256 samples
2026-02-04 01:48:16.947 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:48:16.948 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 260.39
2026-02-04 01:48:16.948 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:16.948 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 8.486912 MB
2026-02-04 01:48:16.948 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.9.self_attn.k_proj using 256 samples
2026-02-04 01:48:17.292 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.34s
2026-02-04 01:48:17.293 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 76.85
2026-02-04 01:48:17.293 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:17.293 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 2.121728 MB
2026-02-04 01:48:17.293 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.9.self_attn.v_proj using 256 samples
2026-02-04 01:48:17.638 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.34s
2026-02-04 01:48:17.638 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 75.23
2026-02-04 01:48:17.638 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:17.638 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 2.121728 MB
2026-02-04 01:48:17.638 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.9.self_attn.o_proj using 256 samples
2026-02-04 01:48:17.986 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:48:17.987 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 13.50
2026-02-04 01:48:17.987 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:17.987 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 8.486912 MB
2026-02-04 01:48:17.987 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.9.mlp.gate_proj using 256 samples
2026-02-04 01:48:18.345 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.36s
2026-02-04 01:48:18.345 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 732.66
2026-02-04 01:48:18.345 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:18.346 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:48:18.346 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.9.mlp.up_proj using 256 samples
2026-02-04 01:48:18.704 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.36s
2026-02-04 01:48:18.704 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 601.88
2026-02-04 01:48:18.704 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:18.704 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:48:18.704 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.9.mlp.down_proj using 256 samples
2026-02-04 01:48:19.424 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.72s
2026-02-04 01:48:19.424 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 7.73
2026-02-04 01:48:19.425 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 7.20% | total memory: 17 GB
2026-02-04 01:48:19.425 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:48:19.425 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False
2026-02-04 01:48:21.384 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-02-04 01:48:21.384 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.10.self_attn.q_proj using 256 samples
2026-02-04 01:48:21.740 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.36s
2026-02-04 01:48:21.740 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 282.98
2026-02-04 01:48:21.761 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:21.761 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 8.486912 MB
2026-02-04 01:48:21.761 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.10.self_attn.k_proj using 256 samples
2026-02-04 01:48:22.107 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:48:22.107 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 76.14
2026-02-04 01:48:22.107 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:22.107 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 2.121728 MB
2026-02-04 01:48:22.108 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.10.self_attn.v_proj using 256 samples
2026-02-04 01:48:22.453 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:48:22.453 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 81.20
2026-02-04 01:48:22.454 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:22.454 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 2.121728 MB
2026-02-04 01:48:22.454 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.10.self_attn.o_proj using 256 samples
2026-02-04 01:48:22.805 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:48:22.805 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 11.75
2026-02-04 01:48:22.806 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:22.806 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 8.486912 MB
2026-02-04 01:48:22.806 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.10.mlp.gate_proj using 256 samples
2026-02-04 01:48:23.178 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.37s
2026-02-04 01:48:23.179 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 888.95
2026-02-04 01:48:23.179 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:23.179 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:48:23.179 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.10.mlp.up_proj using 256 samples
2026-02-04 01:48:23.551 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.37s
2026-02-04 01:48:23.552 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 672.25
2026-02-04 01:48:23.552 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:23.552 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:48:23.552 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.10.mlp.down_proj using 256 samples
2026-02-04 01:48:24.285 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.73s
2026-02-04 01:48:24.286 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 9.17
2026-02-04 01:48:24.286 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 7.20% | total memory: 17 GB
2026-02-04 01:48:24.286 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:48:24.286 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False
2026-02-04 01:48:26.250 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-02-04 01:48:26.250 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.11.self_attn.q_proj using 256 samples
2026-02-04 01:48:26.611 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.36s
2026-02-04 01:48:26.611 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 308.38
2026-02-04 01:48:26.611 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:26.611 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 8.486912 MB
2026-02-04 01:48:26.612 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.11.self_attn.k_proj using 256 samples
2026-02-04 01:48:26.958 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:48:26.979 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 87.18
2026-02-04 01:48:26.979 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:26.980 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 2.121728 MB
2026-02-04 01:48:26.980 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.11.self_attn.v_proj using 256 samples
2026-02-04 01:48:27.326 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:48:27.327 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 93.04
2026-02-04 01:48:27.327 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:27.327 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 2.121728 MB
2026-02-04 01:48:27.327 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.11.self_attn.o_proj using 256 samples
2026-02-04 01:48:27.680 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:48:27.680 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 17.87
2026-02-04 01:48:27.680 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:27.680 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 8.486912 MB
2026-02-04 01:48:27.680 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.11.mlp.gate_proj using 256 samples
2026-02-04 01:48:28.053 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.37s
2026-02-04 01:48:28.053 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 814.16
2026-02-04 01:48:28.053 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:28.054 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:48:28.054 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.11.mlp.up_proj using 256 samples
2026-02-04 01:48:28.426 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.37s
2026-02-04 01:48:28.426 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 717.68
2026-02-04 01:48:28.426 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:28.427 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:48:28.427 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.11.mlp.down_proj using 256 samples
2026-02-04 01:48:29.163 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.74s
2026-02-04 01:48:29.163 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 8.48
2026-02-04 01:48:29.163 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 7.20% | total memory: 17 GB
2026-02-04 01:48:29.163 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:48:29.163 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False
2026-02-04 01:48:31.134 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-02-04 01:48:31.134 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.12.self_attn.q_proj using 256 samples
2026-02-04 01:48:31.496 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.36s
2026-02-04 01:48:31.497 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 345.37
2026-02-04 01:48:31.497 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:31.497 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 8.486912 MB
2026-02-04 01:48:31.497 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.12.self_attn.k_proj using 256 samples
2026-02-04 01:48:31.843 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:48:31.843 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 94.75
2026-02-04 01:48:31.843 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:31.843 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 2.121728 MB
2026-02-04 01:48:31.843 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.12.self_attn.v_proj using 256 samples
2026-02-04 01:48:32.190 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:48:32.191 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 99.28
2026-02-04 01:48:32.211 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:32.211 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 2.121728 MB
2026-02-04 01:48:32.211 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.12.self_attn.o_proj using 256 samples
2026-02-04 01:48:32.563 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:48:32.563 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 14.49
2026-02-04 01:48:32.563 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:32.563 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 8.486912 MB
2026-02-04 01:48:32.563 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.12.mlp.gate_proj using 256 samples
2026-02-04 01:48:32.935 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.37s
2026-02-04 01:48:32.935 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 868.69
2026-02-04 01:48:32.935 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:32.935 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:48:32.935 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.12.mlp.up_proj using 256 samples
2026-02-04 01:48:33.307 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.37s
2026-02-04 01:48:33.307 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 807.67
2026-02-04 01:48:33.307 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:33.308 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:48:33.308 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.12.mlp.down_proj using 256 samples
2026-02-04 01:48:34.042 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.73s
2026-02-04 01:48:34.043 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 10.98
2026-02-04 01:48:34.043 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 7.20% | total memory: 17 GB
2026-02-04 01:48:34.043 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:48:34.043 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False
2026-02-04 01:48:36.016 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-02-04 01:48:36.016 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.13.self_attn.q_proj using 256 samples
2026-02-04 01:48:36.377 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.36s
2026-02-04 01:48:36.377 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 387.30
2026-02-04 01:48:36.377 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:36.377 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 8.486912 MB
2026-02-04 01:48:36.377 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.13.self_attn.k_proj using 256 samples
2026-02-04 01:48:36.723 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:48:36.723 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 108.49
2026-02-04 01:48:36.724 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:36.724 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 2.121728 MB
2026-02-04 01:48:36.724 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.13.self_attn.v_proj using 256 samples
2026-02-04 01:48:37.069 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:48:37.069 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 149.10
2026-02-04 01:48:37.070 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:37.070 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 2.121728 MB
2026-02-04 01:48:37.070 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.13.self_attn.o_proj using 256 samples
2026-02-04 01:48:37.422 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:48:37.423 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 44.40
2026-02-04 01:48:37.423 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:37.423 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 8.486912 MB
2026-02-04 01:48:37.423 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.13.mlp.gate_proj using 256 samples
2026-02-04 01:48:37.794 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.37s
2026-02-04 01:48:37.794 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 923.88
2026-02-04 01:48:37.794 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:37.794 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:48:37.794 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.13.mlp.up_proj using 256 samples
2026-02-04 01:48:38.166 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.37s
2026-02-04 01:48:38.166 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 876.60
2026-02-04 01:48:38.166 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:38.166 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:48:38.167 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.13.mlp.down_proj using 256 samples
2026-02-04 01:48:38.900 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.73s
2026-02-04 01:48:38.901 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 12.83
2026-02-04 01:48:38.901 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 7.20% | total memory: 17 GB
2026-02-04 01:48:38.901 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:48:38.901 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False
2026-02-04 01:48:40.866 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-02-04 01:48:40.866 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.14.self_attn.q_proj using 256 samples
2026-02-04 01:48:41.231 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.37s
2026-02-04 01:48:41.232 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 419.51
2026-02-04 01:48:41.232 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:41.232 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 8.486912 MB
2026-02-04 01:48:41.232 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.14.self_attn.k_proj using 256 samples
2026-02-04 01:48:41.579 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:48:41.579 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 126.03
2026-02-04 01:48:41.579 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:41.579 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 2.121728 MB
2026-02-04 01:48:41.579 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.14.self_attn.v_proj using 256 samples
2026-02-04 01:48:41.925 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:48:41.926 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 119.46
2026-02-04 01:48:41.926 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:41.926 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 2.121728 MB
2026-02-04 01:48:41.926 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.14.self_attn.o_proj using 256 samples
2026-02-04 01:48:42.278 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:48:42.278 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 27.63
2026-02-04 01:48:42.278 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:42.278 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 8.486912 MB
2026-02-04 01:48:42.278 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.14.mlp.gate_proj using 256 samples
2026-02-04 01:48:42.651 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.37s
2026-02-04 01:48:42.651 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 948.73
2026-02-04 01:48:42.651 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:42.671 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:48:42.672 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.14.mlp.up_proj using 256 samples
2026-02-04 01:48:43.045 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.37s
2026-02-04 01:48:43.045 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 960.34
2026-02-04 01:48:43.045 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:43.045 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:48:43.045 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.14.mlp.down_proj using 256 samples
2026-02-04 01:48:43.783 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.74s
2026-02-04 01:48:43.783 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 13.42
2026-02-04 01:48:43.783 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 7.20% | total memory: 17 GB
2026-02-04 01:48:43.783 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:48:43.784 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False
2026-02-04 01:48:45.756 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-02-04 01:48:45.756 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.15.self_attn.q_proj using 256 samples
2026-02-04 01:48:46.121 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.37s
2026-02-04 01:48:46.122 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 434.16
2026-02-04 01:48:46.122 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:46.122 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 8.486912 MB
2026-02-04 01:48:46.122 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.15.self_attn.k_proj using 256 samples
2026-02-04 01:48:46.470 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:48:46.470 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 122.26
2026-02-04 01:48:46.470 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:46.470 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 2.121728 MB
2026-02-04 01:48:46.470 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.15.self_attn.v_proj using 256 samples
2026-02-04 01:48:46.818 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:48:46.818 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 123.22
2026-02-04 01:48:46.818 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:46.818 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 2.121728 MB
2026-02-04 01:48:46.818 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.15.self_attn.o_proj using 256 samples
2026-02-04 01:48:47.171 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:48:47.171 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 18.68
2026-02-04 01:48:47.171 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:47.171 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 8.486912 MB
2026-02-04 01:48:47.171 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.15.mlp.gate_proj using 256 samples
2026-02-04 01:48:47.542 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.37s
2026-02-04 01:48:47.542 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 1020.53
2026-02-04 01:48:47.542 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:47.542 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:48:47.543 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.15.mlp.up_proj using 256 samples
2026-02-04 01:48:47.913 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.37s
2026-02-04 01:48:47.914 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 1059.53
2026-02-04 01:48:47.934 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:47.934 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:48:47.934 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.15.mlp.down_proj using 256 samples
2026-02-04 01:48:48.668 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.73s
2026-02-04 01:48:48.669 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 15.06
2026-02-04 01:48:48.669 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 7.20% | total memory: 17 GB
2026-02-04 01:48:48.669 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:48:48.669 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False
2026-02-04 01:48:50.639 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-02-04 01:48:50.639 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.16.self_attn.q_proj using 256 samples
2026-02-04 01:48:51.001 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.36s
2026-02-04 01:48:51.001 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 514.09
2026-02-04 01:48:51.001 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:51.002 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 8.486912 MB
2026-02-04 01:48:51.002 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.16.self_attn.k_proj using 256 samples
2026-02-04 01:48:51.371 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.37s
2026-02-04 01:48:51.371 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 134.48
2026-02-04 01:48:51.371 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:51.372 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 2.121728 MB
2026-02-04 01:48:51.372 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.16.self_attn.v_proj using 256 samples
2026-02-04 01:48:51.719 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:48:51.719 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 136.12
2026-02-04 01:48:51.719 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:51.719 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 2.121728 MB
2026-02-04 01:48:51.719 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.16.self_attn.o_proj using 256 samples
2026-02-04 01:48:52.072 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:48:52.073 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 16.71
2026-02-04 01:48:52.073 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:52.073 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 8.486912 MB
2026-02-04 01:48:52.073 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.16.mlp.gate_proj using 256 samples
2026-02-04 01:48:52.444 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.37s
2026-02-04 01:48:52.444 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 1088.13
2026-02-04 01:48:52.445 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:52.445 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:48:52.445 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.16.mlp.up_proj using 256 samples
2026-02-04 01:48:52.816 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.37s
2026-02-04 01:48:52.817 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 1172.62
2026-02-04 01:48:52.817 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:52.817 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:48:52.817 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.16.mlp.down_proj using 256 samples
2026-02-04 01:48:53.554 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.74s
2026-02-04 01:48:53.554 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 17.63
2026-02-04 01:48:53.554 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 7.20% | total memory: 17 GB
2026-02-04 01:48:53.554 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:48:53.554 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False
2026-02-04 01:48:55.548 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-02-04 01:48:55.548 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.17.self_attn.q_proj using 256 samples
2026-02-04 01:48:55.909 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.36s
2026-02-04 01:48:55.909 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 532.71
2026-02-04 01:48:55.910 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:55.910 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 8.486912 MB
2026-02-04 01:48:55.910 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.17.self_attn.k_proj using 256 samples
2026-02-04 01:48:56.256 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:48:56.257 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 144.48
2026-02-04 01:48:56.257 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:56.257 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 2.121728 MB
2026-02-04 01:48:56.257 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.17.self_attn.v_proj using 256 samples
2026-02-04 01:48:56.603 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:48:56.604 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 164.50
2026-02-04 01:48:56.604 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:56.604 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 2.121728 MB
2026-02-04 01:48:56.604 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.17.self_attn.o_proj using 256 samples
2026-02-04 01:48:56.955 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:48:56.955 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 14.14
2026-02-04 01:48:56.955 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:56.955 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 8.486912 MB
2026-02-04 01:48:56.955 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.17.mlp.gate_proj using 256 samples
2026-02-04 01:48:57.323 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.37s
2026-02-04 01:48:57.324 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 1100.41
2026-02-04 01:48:57.324 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:57.324 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:48:57.324 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.17.mlp.up_proj using 256 samples
2026-02-04 01:48:57.693 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.37s
2026-02-04 01:48:57.693 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 1213.18
2026-02-04 01:48:57.693 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:48:57.693 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:48:57.693 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.17.mlp.down_proj using 256 samples
2026-02-04 01:48:58.425 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.73s
2026-02-04 01:48:58.426 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 18.07
2026-02-04 01:48:58.426 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 7.20% | total memory: 17 GB
2026-02-04 01:48:58.426 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:48:58.426 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False
2026-02-04 01:49:00.395 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-02-04 01:49:00.395 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.18.self_attn.q_proj using 256 samples
2026-02-04 01:49:00.756 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.36s
2026-02-04 01:49:00.756 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 586.04
2026-02-04 01:49:00.756 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:00.756 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 8.486912 MB
2026-02-04 01:49:00.756 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.18.self_attn.k_proj using 256 samples
2026-02-04 01:49:01.102 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:49:01.102 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 166.52
2026-02-04 01:49:01.102 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:01.102 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 2.121728 MB
2026-02-04 01:49:01.102 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.18.self_attn.v_proj using 256 samples
2026-02-04 01:49:01.456 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:49:01.457 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 165.44
2026-02-04 01:49:01.457 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:01.457 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 2.121728 MB
2026-02-04 01:49:01.457 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.18.self_attn.o_proj using 256 samples
2026-02-04 01:49:01.820 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.36s
2026-02-04 01:49:01.821 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 20.90
2026-02-04 01:49:01.821 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:01.821 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 8.486912 MB
2026-02-04 01:49:01.821 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.18.mlp.gate_proj using 256 samples
2026-02-04 01:49:02.191 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.37s
2026-02-04 01:49:02.192 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 1211.34
2026-02-04 01:49:02.192 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:02.192 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:49:02.192 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.18.mlp.up_proj using 256 samples
2026-02-04 01:49:02.562 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.37s
2026-02-04 01:49:02.562 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 1376.85
2026-02-04 01:49:02.562 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:02.562 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:49:02.562 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.18.mlp.down_proj using 256 samples
2026-02-04 01:49:03.294 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.73s
2026-02-04 01:49:03.295 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 23.78
2026-02-04 01:49:03.295 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 7.20% | total memory: 17 GB
2026-02-04 01:49:03.295 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:49:03.295 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False
2026-02-04 01:49:05.263 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-02-04 01:49:05.263 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.19.self_attn.q_proj using 256 samples
2026-02-04 01:49:05.622 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.36s
2026-02-04 01:49:05.622 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 589.45
2026-02-04 01:49:05.622 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:05.623 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 8.486912 MB
2026-02-04 01:49:05.623 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.19.self_attn.k_proj using 256 samples
2026-02-04 01:49:05.968 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:49:05.969 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 168.33
2026-02-04 01:49:05.969 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:05.969 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 2.121728 MB
2026-02-04 01:49:05.969 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.19.self_attn.v_proj using 256 samples
2026-02-04 01:49:06.315 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:49:06.315 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 185.03
2026-02-04 01:49:06.315 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:06.315 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 2.121728 MB
2026-02-04 01:49:06.315 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.19.self_attn.o_proj using 256 samples
2026-02-04 01:49:06.667 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:49:06.667 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 32.40
2026-02-04 01:49:06.668 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:06.668 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 8.486912 MB
2026-02-04 01:49:06.668 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.19.mlp.gate_proj using 256 samples
2026-02-04 01:49:07.037 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.37s
2026-02-04 01:49:07.038 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 1373.26
2026-02-04 01:49:07.038 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:07.038 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:49:07.038 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.19.mlp.up_proj using 256 samples
2026-02-04 01:49:07.410 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.37s
2026-02-04 01:49:07.410 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 1520.69
2026-02-04 01:49:07.410 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:07.410 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:49:07.410 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.19.mlp.down_proj using 256 samples
2026-02-04 01:49:08.145 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.73s
2026-02-04 01:49:08.145 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 31.82
2026-02-04 01:49:08.145 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 7.20% | total memory: 17 GB
2026-02-04 01:49:08.145 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:49:08.146 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False
2026-02-04 01:49:10.114 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-02-04 01:49:10.114 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.20.self_attn.q_proj using 256 samples
2026-02-04 01:49:10.476 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.36s
2026-02-04 01:49:10.477 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 698.25
2026-02-04 01:49:10.477 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:10.477 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 8.486912 MB
2026-02-04 01:49:10.477 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.20.self_attn.k_proj using 256 samples
2026-02-04 01:49:10.823 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:49:10.823 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 186.90
2026-02-04 01:49:10.823 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:10.824 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 2.121728 MB
2026-02-04 01:49:10.824 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.20.self_attn.v_proj using 256 samples
2026-02-04 01:49:11.169 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:49:11.170 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 212.22
2026-02-04 01:49:11.170 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:11.170 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 2.121728 MB
2026-02-04 01:49:11.170 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.20.self_attn.o_proj using 256 samples
2026-02-04 01:49:11.523 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:49:11.523 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 32.23
2026-02-04 01:49:11.523 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:11.523 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 8.486912 MB
2026-02-04 01:49:11.523 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.20.mlp.gate_proj using 256 samples
2026-02-04 01:49:11.919 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.40s
2026-02-04 01:49:11.920 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 1564.84
2026-02-04 01:49:11.920 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:11.920 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:49:11.920 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.20.mlp.up_proj using 256 samples
2026-02-04 01:49:12.290 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.37s
2026-02-04 01:49:12.291 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 1702.85
2026-02-04 01:49:12.291 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:12.291 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:49:12.291 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.20.mlp.down_proj using 256 samples
2026-02-04 01:49:13.025 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.73s
2026-02-04 01:49:13.025 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 40.65
2026-02-04 01:49:13.025 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 7.20% | total memory: 17 GB
2026-02-04 01:49:13.025 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:49:13.026 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False
2026-02-04 01:49:14.999 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-02-04 01:49:14.999 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.21.self_attn.q_proj using 256 samples
2026-02-04 01:49:15.361 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.36s
2026-02-04 01:49:15.362 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 802.14
2026-02-04 01:49:15.362 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:15.362 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 8.486912 MB
2026-02-04 01:49:15.362 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.21.self_attn.k_proj using 256 samples
2026-02-04 01:49:15.709 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:49:15.709 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 214.83
2026-02-04 01:49:15.709 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:15.710 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 2.121728 MB
2026-02-04 01:49:15.710 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.21.self_attn.v_proj using 256 samples
2026-02-04 01:49:16.057 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:49:16.057 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 216.95
2026-02-04 01:49:16.057 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:16.057 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 2.121728 MB
2026-02-04 01:49:16.057 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.21.self_attn.o_proj using 256 samples
2026-02-04 01:49:16.414 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.36s
2026-02-04 01:49:16.414 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 29.97
2026-02-04 01:49:16.414 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:16.414 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 8.486912 MB
2026-02-04 01:49:16.414 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.21.mlp.gate_proj using 256 samples
2026-02-04 01:49:16.784 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.37s
2026-02-04 01:49:16.784 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 1833.54
2026-02-04 01:49:16.784 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:16.784 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:49:16.785 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.21.mlp.up_proj using 256 samples
2026-02-04 01:49:17.154 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.37s
2026-02-04 01:49:17.155 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 2025.62
2026-02-04 01:49:17.155 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:17.155 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:49:17.155 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.21.mlp.down_proj using 256 samples
2026-02-04 01:49:17.887 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.73s
2026-02-04 01:49:17.888 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 56.08
2026-02-04 01:49:17.888 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 7.20% | total memory: 17 GB
2026-02-04 01:49:17.888 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:49:17.888 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False
2026-02-04 01:49:19.857 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-02-04 01:49:19.857 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.22.self_attn.q_proj using 256 samples
2026-02-04 01:49:20.217 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.36s
2026-02-04 01:49:20.217 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 875.41
2026-02-04 01:49:20.218 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:20.218 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 8.486912 MB
2026-02-04 01:49:20.218 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.22.self_attn.k_proj using 256 samples
2026-02-04 01:49:20.564 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:49:20.564 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 247.59
2026-02-04 01:49:20.564 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:20.564 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 2.121728 MB
2026-02-04 01:49:20.565 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.22.self_attn.v_proj using 256 samples
2026-02-04 01:49:20.910 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:49:20.910 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 285.91
2026-02-04 01:49:20.911 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:20.911 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 2.121728 MB
2026-02-04 01:49:20.911 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.22.self_attn.o_proj using 256 samples
2026-02-04 01:49:21.262 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:49:21.262 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 44.16
2026-02-04 01:49:21.262 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:21.262 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 8.486912 MB
2026-02-04 01:49:21.263 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.22.mlp.gate_proj using 256 samples
2026-02-04 01:49:21.632 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.37s
2026-02-04 01:49:21.633 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 2438.65
2026-02-04 01:49:21.633 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:21.633 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:49:21.633 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.22.mlp.up_proj using 256 samples
2026-02-04 01:49:22.004 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.37s
2026-02-04 01:49:22.004 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 2414.73
2026-02-04 01:49:22.004 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:22.004 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:49:22.004 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.22.mlp.down_proj using 256 samples
2026-02-04 01:49:22.749 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.74s
2026-02-04 01:49:22.749 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 82.86
2026-02-04 01:49:22.749 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 7.20% | total memory: 17 GB
2026-02-04 01:49:22.749 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:49:22.749 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False
2026-02-04 01:49:24.720 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-02-04 01:49:24.720 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.23.self_attn.q_proj using 256 samples
2026-02-04 01:49:25.080 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.36s
2026-02-04 01:49:25.081 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 972.91
2026-02-04 01:49:25.081 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:25.081 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 8.486912 MB
2026-02-04 01:49:25.081 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.23.self_attn.k_proj using 256 samples
2026-02-04 01:49:25.426 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:49:25.427 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 286.76
2026-02-04 01:49:25.427 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:25.427 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 2.121728 MB
2026-02-04 01:49:25.427 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.23.self_attn.v_proj using 256 samples
2026-02-04 01:49:25.772 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:49:25.772 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 369.56
2026-02-04 01:49:25.773 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:25.773 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 2.121728 MB
2026-02-04 01:49:25.773 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.23.self_attn.o_proj using 256 samples
2026-02-04 01:49:26.124 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:49:26.125 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 76.84
2026-02-04 01:49:26.125 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:26.125 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 8.486912 MB
2026-02-04 01:49:26.125 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.23.mlp.gate_proj using 256 samples
2026-02-04 01:49:26.496 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.37s
2026-02-04 01:49:26.496 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 2745.45
2026-02-04 01:49:26.496 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:26.496 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:49:26.496 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.23.mlp.up_proj using 256 samples
2026-02-04 01:49:26.866 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.37s
2026-02-04 01:49:26.867 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 2911.06
2026-02-04 01:49:26.867 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:26.867 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:49:26.867 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.23.mlp.down_proj using 256 samples
2026-02-04 01:49:27.600 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.73s
2026-02-04 01:49:27.601 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 146.04
2026-02-04 01:49:27.601 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 7.20% | total memory: 17 GB
2026-02-04 01:49:27.601 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:49:27.601 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False
2026-02-04 01:49:29.568 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-02-04 01:49:29.568 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.24.self_attn.q_proj using 256 samples
2026-02-04 01:49:29.930 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.36s
2026-02-04 01:49:29.930 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 1388.68
2026-02-04 01:49:29.930 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:29.930 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 8.486912 MB
2026-02-04 01:49:29.930 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.24.self_attn.k_proj using 256 samples
2026-02-04 01:49:30.276 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:49:30.277 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 369.34
2026-02-04 01:49:30.297 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:30.297 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 2.121728 MB
2026-02-04 01:49:30.297 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.24.self_attn.v_proj using 256 samples
2026-02-04 01:49:30.644 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:49:30.644 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 446.39
2026-02-04 01:49:30.645 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:30.645 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 2.121728 MB
2026-02-04 01:49:30.645 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.24.self_attn.o_proj using 256 samples
2026-02-04 01:49:30.996 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:49:30.996 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 41.22
2026-02-04 01:49:30.996 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:30.996 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 8.486912 MB
2026-02-04 01:49:30.996 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.24.mlp.gate_proj using 256 samples
2026-02-04 01:49:31.367 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.37s
2026-02-04 01:49:31.367 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 2808.96
2026-02-04 01:49:31.367 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:31.367 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:49:31.367 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.24.mlp.up_proj using 256 samples
2026-02-04 01:49:31.736 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.37s
2026-02-04 01:49:31.736 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 3556.16
2026-02-04 01:49:31.736 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:31.736 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:49:31.736 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.24.mlp.down_proj using 256 samples
2026-02-04 01:49:32.486 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.75s
2026-02-04 01:49:32.487 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 184.89
2026-02-04 01:49:32.487 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 7.20% | total memory: 17 GB
2026-02-04 01:49:32.487 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:49:32.487 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False
2026-02-04 01:49:34.455 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-02-04 01:49:34.455 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.25.self_attn.q_proj using 256 samples
2026-02-04 01:49:34.816 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.36s
2026-02-04 01:49:34.816 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 1581.13
2026-02-04 01:49:34.816 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:34.816 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 8.486912 MB
2026-02-04 01:49:34.816 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.25.self_attn.k_proj using 256 samples
2026-02-04 01:49:35.162 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:49:35.162 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 400.13
2026-02-04 01:49:35.163 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:35.163 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 2.121728 MB
2026-02-04 01:49:35.163 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.25.self_attn.v_proj using 256 samples
2026-02-04 01:49:35.509 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:49:35.509 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 639.36
2026-02-04 01:49:35.509 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:35.509 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 2.121728 MB
2026-02-04 01:49:35.509 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.25.self_attn.o_proj using 256 samples
2026-02-04 01:49:35.860 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:49:35.861 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 36.83
2026-02-04 01:49:35.861 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:35.861 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 8.486912 MB
2026-02-04 01:49:35.861 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.25.mlp.gate_proj using 256 samples
2026-02-04 01:49:36.233 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.37s
2026-02-04 01:49:36.234 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 3378.03
2026-02-04 01:49:36.234 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:36.234 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:49:36.234 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.25.mlp.up_proj using 256 samples
2026-02-04 01:49:36.606 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.37s
2026-02-04 01:49:36.606 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 4334.75
2026-02-04 01:49:36.606 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:36.606 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:49:36.606 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.25.mlp.down_proj using 256 samples
2026-02-04 01:49:37.341 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.73s
2026-02-04 01:49:37.341 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 253.09
2026-02-04 01:49:37.342 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 7.20% | total memory: 17 GB
2026-02-04 01:49:37.342 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:49:37.342 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False
2026-02-04 01:49:39.309 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-02-04 01:49:39.309 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.26.self_attn.q_proj using 256 samples
2026-02-04 01:49:39.670 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.36s
2026-02-04 01:49:39.670 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 1890.77
2026-02-04 01:49:39.670 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:39.670 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 8.486912 MB
2026-02-04 01:49:39.670 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.26.self_attn.k_proj using 256 samples
2026-02-04 01:49:40.016 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:49:40.016 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 511.53
2026-02-04 01:49:40.017 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:40.017 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 2.121728 MB
2026-02-04 01:49:40.017 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.26.self_attn.v_proj using 256 samples
2026-02-04 01:49:40.363 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:49:40.363 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 739.82
2026-02-04 01:49:40.363 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:40.363 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 2.121728 MB
2026-02-04 01:49:40.363 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.26.self_attn.o_proj using 256 samples
2026-02-04 01:49:40.714 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:49:40.715 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 31.52
2026-02-04 01:49:40.735 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:40.735 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 8.486912 MB
2026-02-04 01:49:40.735 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.26.mlp.gate_proj using 256 samples
2026-02-04 01:49:41.104 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.37s
2026-02-04 01:49:41.104 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 4158.57
2026-02-04 01:49:41.104 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:41.104 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:49:41.104 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.26.mlp.up_proj using 256 samples
2026-02-04 01:49:41.474 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.37s
2026-02-04 01:49:41.475 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 5345.57
2026-02-04 01:49:41.475 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:41.475 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:49:41.475 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.26.mlp.down_proj using 256 samples
2026-02-04 01:49:42.209 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.73s
2026-02-04 01:49:42.209 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 396.71
2026-02-04 01:49:42.209 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 7.20% | total memory: 17 GB
2026-02-04 01:49:42.210 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:49:42.210 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False
2026-02-04 01:49:44.178 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-02-04 01:49:44.178 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.27.self_attn.q_proj using 256 samples
2026-02-04 01:49:44.540 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.36s
2026-02-04 01:49:44.540 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 2850.08
2026-02-04 01:49:44.540 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:44.540 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 8.486912 MB
2026-02-04 01:49:44.540 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.27.self_attn.k_proj using 256 samples
2026-02-04 01:49:44.887 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:49:44.887 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 733.42
2026-02-04 01:49:44.887 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:44.888 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 2.121728 MB
2026-02-04 01:49:44.888 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.27.self_attn.v_proj using 256 samples
2026-02-04 01:49:45.234 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:49:45.234 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 1355.27
2026-02-04 01:49:45.234 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:45.234 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 2.121728 MB
2026-02-04 01:49:45.234 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.27.self_attn.o_proj using 256 samples
2026-02-04 01:49:45.587 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:49:45.587 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 34.91
2026-02-04 01:49:45.587 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:45.587 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 8.486912 MB
2026-02-04 01:49:45.587 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.27.mlp.gate_proj using 256 samples
2026-02-04 01:49:45.965 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.38s
2026-02-04 01:49:45.965 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 5240.39
2026-02-04 01:49:45.986 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:45.986 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:49:45.986 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.27.mlp.up_proj using 256 samples
2026-02-04 01:49:46.356 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.37s
2026-02-04 01:49:46.356 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 6965.94
2026-02-04 01:49:46.357 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:46.357 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:49:46.357 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.27.mlp.down_proj using 256 samples
2026-02-04 01:49:47.089 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.73s
2026-02-04 01:49:47.089 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 646.72
2026-02-04 01:49:47.089 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 7.20% | total memory: 17 GB
2026-02-04 01:49:47.089 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:49:47.090 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False
2026-02-04 01:49:49.052 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-02-04 01:49:49.053 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.28.self_attn.q_proj using 256 samples
2026-02-04 01:49:49.415 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.36s
2026-02-04 01:49:49.415 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 3271.79
2026-02-04 01:49:49.415 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:49.415 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 8.486912 MB
2026-02-04 01:49:49.415 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.28.self_attn.k_proj using 256 samples
2026-02-04 01:49:49.764 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:49:49.765 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 844.46
2026-02-04 01:49:49.765 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:49.765 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 2.121728 MB
2026-02-04 01:49:49.765 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.28.self_attn.v_proj using 256 samples
2026-02-04 01:49:50.110 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:49:50.110 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 1718.92
2026-02-04 01:49:50.110 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:50.111 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 2.121728 MB
2026-02-04 01:49:50.111 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.28.self_attn.o_proj using 256 samples
2026-02-04 01:49:50.463 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:49:50.464 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 65.57
2026-02-04 01:49:50.464 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:50.464 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 8.486912 MB
2026-02-04 01:49:50.464 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.28.mlp.gate_proj using 256 samples
2026-02-04 01:49:50.837 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.37s
2026-02-04 01:49:50.838 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 8963.25
2026-02-04 01:49:50.838 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:50.838 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:49:50.838 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.28.mlp.up_proj using 256 samples
2026-02-04 01:49:51.206 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.37s
2026-02-04 01:49:51.206 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 10185.68
2026-02-04 01:49:51.207 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:51.207 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:49:51.207 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.28.mlp.down_proj using 256 samples
2026-02-04 01:49:51.941 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.73s
2026-02-04 01:49:51.942 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 2209.40
2026-02-04 01:49:51.942 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 7.20% | total memory: 17 GB
2026-02-04 01:49:51.942 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:49:51.942 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False
2026-02-04 01:49:53.914 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-02-04 01:49:53.914 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.29.self_attn.q_proj using 256 samples
2026-02-04 01:49:54.275 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.36s
2026-02-04 01:49:54.276 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 3246.09
2026-02-04 01:49:54.276 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:54.276 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 8.486912 MB
2026-02-04 01:49:54.276 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.29.self_attn.k_proj using 256 samples
2026-02-04 01:49:54.622 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:49:54.622 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 920.19
2026-02-04 01:49:54.622 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:54.622 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 2.121728 MB
2026-02-04 01:49:54.622 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.29.self_attn.v_proj using 256 samples
2026-02-04 01:49:54.968 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:49:54.968 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 1391.63
2026-02-04 01:49:54.968 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:54.968 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 2.121728 MB
2026-02-04 01:49:54.968 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.29.self_attn.o_proj using 256 samples
2026-02-04 01:49:55.320 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.35s
2026-02-04 01:49:55.320 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 44.25
2026-02-04 01:49:55.320 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:55.320 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 8.486912 MB
2026-02-04 01:49:55.320 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.29.mlp.gate_proj using 256 samples
2026-02-04 01:49:55.692 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.37s
2026-02-04 01:49:55.693 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 7381.71
2026-02-04 01:49:55.693 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:55.693 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:49:55.693 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.29.mlp.up_proj using 256 samples
2026-02-04 01:49:56.064 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.37s
2026-02-04 01:49:56.065 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 6408.42
2026-02-04 01:49:56.065 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 6.81% | total memory: 17 GB
2026-02-04 01:49:56.065 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:49:56.065 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:251 - Quantizing model.layers.29.mlp.down_proj using 256 samples
2026-02-04 01:49:56.799 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - time 0.73s
2026-02-04 01:49:56.799 | METRIC   | llmcompressor.utils.metric_logging:compress:138 - error 678.48
2026-02-04 01:49:56.799 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - GPU 0 | usage: 7.20% | total memory: 17 GB
2026-02-04 01:49:56.819 | METRIC   | llmcompressor.utils.metric_logging:compress:154 - Compressed module size: 16.973824 MB
2026-02-04 01:49:56.820 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False
2026-02-04 01:49:58.416 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-02-04 01:49:58.416 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False
2026-02-04 01:49:59.732 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.CALIBRATION_EPOCH_END
2026-02-04 01:49:59.735 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False
2026-02-04 01:49:59.737 | DEBUG    | llmcompressor.core.lifecycle:finalize:134 - Finalizing compression lifecycle
2026-02-04 01:49:59.737 | DEBUG    | llmcompressor.core.lifecycle:finalize:138 - Finalized modifier: config_groups=None targets=['Linear'] ignore=['embed_tokens', 'lm_head'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=True started_=True ended_=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=static offload_hessians=False
2026-02-04 01:49:59.737 | INFO     | llmcompressor.core.lifecycle:finalize:144 - Compression lifecycle finalized for 1 modifiers
2026-02-04 01:49:59.753 | WARNING  | llmcompressor.entrypoints.utils:post_process:142 - Optimized model is not saved. To save, please provide`output_dir` as input arg.Ex. `oneshot(..., output_dir=...)`
2026-02-04 01:49:59.765 | INFO     | llmcompressor.transformers.compression.compressed_tensors_utils:get_model_compressor:193 - skip_sparsity_compression_stats set to True. Skipping sparsity compression statistic calculations. No sparsity compressor will be applied.
2026-02-04 01:50:02.301 | DEBUG    | llmcompressor.transformers.utils.helpers:infer_recipe_from_model_path:105 - No recipe found in the model_path: ./base_model
